{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6f921f-2a41-4232-939c-b6965a222b6f",
   "metadata": {},
   "source": [
    "## Preprocess FS Jump3D\n",
    "- 2D data from DWposeDetector\n",
    "- 3D data from json file formatted to h36m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb4cfe6-baa4-4899-b216-348cef799d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dwpose.scripts.dwpose import DWposeDetector\n",
    "from dwpose.scripts.tool import read_frames\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c3fa9e-ca57-45a2-bb57-e5d6a65762e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v2.0/yolox/yolox_l_8x8_300e_coco/yolox_l_8x8_300e_coco_20211126_140236-d3bd2b23.pth\n",
      "Loads checkpoint by http backend from path: https://huggingface.co/wanghaofan/dw-ll_ucoco_384/resolve/main/dw-ll_ucoco_384.pth\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "detector = DWposeDetector(\n",
    "    det_config = \"D:\\\\github\\\\skating-ai\\\\v3\\\\pose\\\\dwpose\\\\config\\\\yolox_l_8xb8-300e_coco.py\",\n",
    "    # det_ckpt = args.yolox_ckpt,\n",
    "    pose_config = \"D:\\\\github\\\\skating-ai\\\\v3\\\\pose\\\\dwpose\\\\config\\\\dwpose-l_384x288.py\",\n",
    "    # pose_ckpt = args.dwpose_ckpt, \n",
    "    keypoints_only=True\n",
    "    )    \n",
    "detector = detector.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbaf93c-5ba9-4945-a57a-b28183125eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def estimate2d(video_path, detector, yolo_model_path='yolov8n.pt'):\n",
    "    # Load YOLO model - force CPU to avoid CUDA issues\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    yolo_model.to('cpu')  # Force CPU inference\n",
    "    \n",
    "    frames = read_frames(video_path)\n",
    "    kpts2d = []\n",
    "    score2d = []\n",
    "    \n",
    "    for idx, frame in enumerate(frames):\n",
    "        # YOLO detection - find largest person (CPU inference)\n",
    "        yolo_results = yolo_model(frame, classes=[0], verbose=False, device='cpu')\n",
    "        \n",
    "        # Pose detection\n",
    "        pose = detector(frame)\n",
    "        candidate = pose[\"bodies\"][\"candidate\"]\n",
    "        subset = pose[\"bodies\"][\"subset\"]\n",
    "        \n",
    "        num_person = subset.shape[0]\n",
    "        num_joints = subset.shape[1]\n",
    "        \n",
    "        if num_person == 0:\n",
    "            # No person detected\n",
    "            kpts2d.append(np.zeros((18, 2)))\n",
    "            score2d.append(np.zeros(18))\n",
    "            continue\n",
    "        \n",
    "        keypoint = candidate.reshape(num_person, num_joints, 2)\n",
    "        \n",
    "        # Find index of person with largest YOLO bbox\n",
    "        person_idx = -1  # default\n",
    "        if len(yolo_results[0].boxes) > 0:\n",
    "            # Get largest bbox by area\n",
    "            largest_area = 0\n",
    "            for i, box in enumerate(yolo_results[0].boxes):\n",
    "                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                area = (x2 - x1) * (y2 - y1)\n",
    "                if area > largest_area:\n",
    "                    largest_area = area\n",
    "                    person_idx = min(i, num_person - 1)  # ensure valid index\n",
    "        \n",
    "        # Use largest person or fallback to last person\n",
    "        if person_idx >= 0:\n",
    "            selected_keypoints = keypoint[person_idx][1:]\n",
    "            selected_scores = subset[person_idx][1:]\n",
    "        else:\n",
    "            selected_keypoints = keypoint[-1][1:]\n",
    "            selected_scores = subset[-1][1:]\n",
    "        \n",
    "        kpts2d.append(selected_keypoints)\n",
    "        score2d.append(selected_scores)\n",
    "    \n",
    "    kpts2d = np.array(kpts2d)\n",
    "    score2d = np.expand_dims(np.array(score2d), axis=-1)\n",
    "    keypoints = np.concatenate([kpts2d, score2d], axis=-1)\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f109a1-a6ca-4956-b523-8b8d1e62b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    \"\"\"Process a single video file\"\"\"\n",
    "    try:\n",
    "        path_parts = Path(video_path).parts\n",
    "        skater = path_parts[-3].lower()\n",
    "        camera = path_parts[-2].lower()\n",
    "        filename = Path(video_path).stem.lower()\n",
    "        output_name = f\"{skater}_{camera}_{filename}_2D.npy\"\n",
    "        \n",
    "        output_path = os.path.join(output_dir, output_name)\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            keypoints = estimate2d(video_path, detector)\n",
    "            np.save(output_path, keypoints)\n",
    "            # Create animation\n",
    "            # anim = FuncAnimation(fig, animate, frames=300, interval=100, repeat=True)\n",
    "            \n",
    "            # To save as gif (optional)\n",
    "            # anim.save(f\"{skater}_{camera}_{filename}.gif\", writer='pillow', fps=10)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {video_path}: {str(e)}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc827a09-4851-4db3-bf0c-ae4a28f6695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = glob.glob(\"D:\\\\github\\\\FS-Jump3D\\\\data\\\\**\\\\*.mp4\", recursive=True)\n",
    "output_dir = \"D:\\\\github\\\\MotionAGFormer\\\\data\\\\keypoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e52d6c-1124-43af-935d-7ebc15aaa210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def animate(frame_num, keypoints):\n",
    "    ax.clear()\n",
    "    \n",
    "    # Get current frame data\n",
    "    frame = keypoints[frame_num]\n",
    "    x = frame[:, 0]\n",
    "    y = frame[:, 1]\n",
    "    \n",
    "    # Plot keypoints\n",
    "    ax.scatter(x, y, s=100, c='red')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f'Frame {frame_num}')\n",
    "    ax.set_xlim(np.min(keypoints[:,:,0]), np.max(keypoints[:,:,0]))\n",
    "    ax.set_ylim(np.max(keypoints[:,:,1]), np.min(keypoints[:,:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598783df-9629-4af4-901e-038235750648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 3036/3036 [15:11:41<00:00, 18.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "max_workers = 4  # Adjust based on your system\n",
    "    \n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Use tqdm for progress bar\n",
    "    futures = [executor.submit(process_video, video_path) for video_path in video_paths]\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(video_paths), desc=\"Processing videos\"):\n",
    "        result = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04166b-4f1a-4e0a-a12e-3022d1fe781e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
